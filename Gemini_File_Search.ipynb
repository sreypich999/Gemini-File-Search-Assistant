{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import time\n",
        "import tempfile\n",
        "import mimetypes\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# -----------------------------\n",
        "# Config & Globals\n",
        "# -----------------------------\n",
        "API_KEY = \"\"\n",
        "CLIENT = None\n",
        "CURRENT_STORE = None\n",
        "FALLBACK_FILE = \"/mnt/data/364d0f88-e8a5-4c4a-8eb2-e71fba0aadc3.png\"\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def _ensure_client(key):\n",
        "    global CLIENT\n",
        "    if CLIENT:\n",
        "        return CLIENT\n",
        "    CLIENT = genai.Client(api_key=key)\n",
        "    return CLIENT\n",
        "\n",
        "def _save_temp(path):\n",
        "    fd, tmp = tempfile.mkstemp(suffix=os.path.splitext(path)[1])\n",
        "    os.close(fd)\n",
        "    with open(path, \"rb\") as f_in, open(tmp, \"wb\") as f_out:\n",
        "        f_out.write(f_in.read())\n",
        "    return tmp\n",
        "\n",
        "# -----------------------------\n",
        "# File Search Flow (from Google doc)\n",
        "# -----------------------------\n",
        "def process_file(file_obj):\n",
        "    global CLIENT, CURRENT_STORE\n",
        "\n",
        "    if CLIENT is None:\n",
        "        return \"⚠️ Please connect API first.\"\n",
        "\n",
        "    src = file_obj.name if (file_obj and file_obj.name) else FALLBACK_FILE\n",
        "    if not os.path.exists(src):\n",
        "        return \"❌ File not found.\"\n",
        "\n",
        "    tmp = _save_temp(src)\n",
        "    try:\n",
        "        # 1) Create a File Search store\n",
        "        store = CLIENT.file_search_stores.create(config={\"display_name\": f\"store_{int(time.time())}\"})\n",
        "        CURRENT_STORE = store\n",
        "\n",
        "        # 2) Upload + import file into that store\n",
        "        op = CLIENT.file_search_stores.upload_to_file_search_store(\n",
        "            file=tmp,\n",
        "            file_search_store_name=store.name,\n",
        "            config={\"display_name\": os.path.basename(src)}\n",
        "        )\n",
        "\n",
        "        # 3) Poll until import is done\n",
        "        while not op.done:\n",
        "            time.sleep(5)\n",
        "            op = CLIENT.operations.get(op)\n",
        "\n",
        "        return f\"✅ File imported into File Search store: {store.name}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {e}\"\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(tmp):\n",
        "            os.remove(tmp)\n",
        "\n",
        "def chat_logic(message, history):\n",
        "    global CLIENT, CURRENT_STORE\n",
        "\n",
        "    if CLIENT is None:\n",
        "        return \"⚠️ Connect API first.\"\n",
        "    if CURRENT_STORE is None:\n",
        "        return \"⚠️ Upload a document first.\"\n",
        "\n",
        "    # Use the FileSearch tool\n",
        "    tool = types.Tool(\n",
        "        file_search=types.FileSearch(\n",
        "            file_search_store_names=[CURRENT_STORE.name]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Build the generate_content config\n",
        "    config = types.GenerateContentConfig(\n",
        "        tools=[tool],\n",
        "        temperature=0.0,\n",
        "        max_output_tokens=800\n",
        "    )\n",
        "\n",
        "    response = CLIENT.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=[message],\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # The docs show response.text\n",
        "    return response.text\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "def setup_client(api_key_input):\n",
        "    key = (api_key_input or \"\").strip() or API_KEY\n",
        "    if not key:\n",
        "        return \"⚠️ Please enter your Gemini API Key\"\n",
        "    try:\n",
        "        _ensure_client(key)\n",
        "        return \"✅ Gemini client connected.\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error connecting: {e}\"\n",
        "\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"## Gemini File Search Assistant (using official API)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            api_input = gr.Textbox(label=\"Gemini API Key\", type=\"password\")\n",
        "            btn = gr.Button(\"Connect\")\n",
        "            status = gr.Markdown(\"Not connected\")\n",
        "\n",
        "            file_input = gr.File(label=\"Upload PDF / Image / Text\")\n",
        "            upload_status = gr.Markdown(\"\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            chat = gr.ChatInterface(fn=chat_logic, type=\"messages\", title=\"Ask about your document\")\n",
        "\n",
        "    btn.click(setup_client, api_input, status)\n",
        "    file_input.change(process_file, file_input, upload_status)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch()\n"
      ],
      "metadata": {
        "id": "CUN2AUnEk4Yk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}